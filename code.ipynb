{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b55fb559",
      "metadata": {},
      "source": [
        "# Short-form Video Search Playground\n",
        "This notebook recreates the core CLIP-based video indexing/search pipeline from the `CLIP-video-search` repository,\n",
        "and adapts it so you can experiment directly on the MSRVTT clips that live under `shortform-video-ranker/data`.\n",
        "\n",
        "Use it end-to-end or cherry-pick the pieces you need for your short-form video + LLM reranking project.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3ad2817",
      "metadata": {},
      "source": [
        "## Environment setup\n",
        "Make sure the runtime has GPU access plus the following system dependencies:\n",
        "- `ffmpeg/ffprobe` (for duration metadata)\n",
        "- Python packages: `torch`, `transformers`, `opencv-python`, `scenedetect`, `tqdm`, `pillow`\n",
        "\n",
        "If anything is missing, uncomment the `%pip` cell below or install the packages however you prefer before running the rest of the notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "50776d25",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Collecting torch\n",
            "  Obtaining dependency information for torch from https://download.pytorch.org/whl/cu118/torch-2.7.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.7.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (28 kB)\n",
            "Collecting torchvision\n",
            "  Obtaining dependency information for torchvision from https://download.pytorch.org/whl/cu118/torchvision-0.22.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.22.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting torchaudio\n",
            "  Obtaining dependency information for torchaudio from https://download.pytorch.org/whl/cu118/torchaudio-2.7.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.7.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting filelock\n",
            "  Obtaining dependency information for filelock from https://download.pytorch.org/whl/filelock-3.19.1-py3-none-any.whl.metadata\n",
            "  Downloading https://download.pytorch.org/whl/filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /home/khushpatel/venv/lib/python3.11/site-packages (from torch) (4.15.0)\n",
            "Collecting sympy>=1.13.3\n",
            "  Obtaining dependency information for sympy>=1.13.3 from https://download.pytorch.org/whl/sympy-1.14.0-py3-none-any.whl.metadata\n",
            "  Downloading https://download.pytorch.org/whl/sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting networkx\n",
            "  Obtaining dependency information for networkx from https://download.pytorch.org/whl/networkx-3.5-py3-none-any.whl.metadata\n",
            "  Downloading https://download.pytorch.org/whl/networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting jinja2\n",
            "  Obtaining dependency information for jinja2 from https://download.pytorch.org/whl/jinja2-3.1.6-py3-none-any.whl.metadata\n",
            "  Downloading https://download.pytorch.org/whl/jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Discarding \u001b[4;34mhttps://download.pytorch.org/whl/jinja2-3.1.6-py3-none-any.whl (from https://download.pytorch.org/whl/cu118/jinja2/)\u001b[0m: \u001b[33mRequested jinja2 from https://download.pytorch.org/whl/jinja2-3.1.6-py3-none-any.whl (from torch) has inconsistent Name: expected 'jinja2', but metadata has 'Jinja2'\u001b[0m\n",
            "  Obtaining dependency information for jinja2 from https://download.pytorch.org/whl/Jinja2-3.1.4-py3-none-any.whl.metadata\n",
            "  Downloading https://download.pytorch.org/whl/Jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
            "Discarding \u001b[4;34mhttps://download.pytorch.org/whl/Jinja2-3.1.4-py3-none-any.whl#sha256=bc5dd2abb727a5319567b7a813e6a2e7318c39f4f487cfe6c89c6f9c7d25197d (from https://download.pytorch.org/whl/cu118/jinja2/)\u001b[0m: \u001b[33mRequested jinja2 from https://download.pytorch.org/whl/Jinja2-3.1.4-py3-none-any.whl#sha256=bc5dd2abb727a5319567b7a813e6a2e7318c39f4f487cfe6c89c6f9c7d25197d (from torch) has inconsistent Name: expected 'jinja2', but metadata has 'Jinja2'\u001b[0m\n",
            "  Downloading https://download.pytorch.org/whl/Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.2/133.2 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fsspec\n",
            "  Obtaining dependency information for fsspec from https://download.pytorch.org/whl/fsspec-2025.9.0-py3-none-any.whl.metadata\n",
            "  Downloading https://download.pytorch.org/whl/fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.8.89\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (23.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.8.89\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.8.87\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==9.1.0.70\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-9.1.0.70-py3-none-manylinux2014_x86_64.whl (663.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m663.9/663.9 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.11.3.6\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.3.0.86\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.1.48\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.5.86\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.21.5\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.21.5-py3-none-manylinux2014_x86_64.whl (147.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.8.86\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nvtx_cu11-11.8.86-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m506.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting triton==3.3.1\n",
            "  Obtaining dependency information for triton==3.3.1 from https://download.pytorch.org/whl/triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata\n",
            "  Downloading https://download.pytorch.org/whl/triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /home/khushpatel/venv/lib/python3.11/site-packages (from triton==3.3.1->torch) (66.1.1)\n",
            "Collecting numpy\n",
            "  Obtaining dependency information for numpy from https://download.pytorch.org/whl/numpy-2.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata\n",
            "  Downloading https://download.pytorch.org/whl/numpy-2.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m225.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting pillow!=8.3.*,>=5.3.0\n",
            "  Obtaining dependency information for pillow!=8.3.*,>=5.3.0 from https://download.pytorch.org/whl/pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata\n",
            "  Downloading https://download.pytorch.org/whl/pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0\n",
            "  Downloading https://download.pytorch.org/whl/mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting MarkupSafe>=2.0\n",
            "  Downloading https://download.pytorch.org/whl/MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
            "Downloading https://download.pytorch.org/whl/cu118/torch-2.7.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl (905.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m905.3/905.3 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.7/155.7 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu118/torchvision-0.22.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl (6.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m107.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu118/torchaudio-2.7.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m101.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m105.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m104.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/filelock-3.19.1-py3-none-any.whl (15 kB)\n",
            "Downloading https://download.pytorch.org/whl/fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.3/199.3 kB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/networkx-3.5-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m94.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/numpy-2.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m97.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hUsing cached https://download.pytorch.org/whl/cu118/torch-2.7.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl (905.3 MB)\n",
            "Using cached https://download.pytorch.org/whl/triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.7 MB)\n",
            "Using cached https://download.pytorch.org/whl/cu118/torchvision-0.22.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl (6.7 MB)\n",
            "Using cached https://download.pytorch.org/whl/cu118/torchaudio-2.7.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl (3.3 MB)\n",
            "Using cached https://download.pytorch.org/whl/pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n",
            "Using cached https://download.pytorch.org/whl/sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "Using cached https://download.pytorch.org/whl/filelock-3.19.1-py3-none-any.whl (15 kB)\n",
            "Using cached https://download.pytorch.org/whl/fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
            "Using cached https://download.pytorch.org/whl/networkx-3.5-py3-none-any.whl (2.0 MB)\n",
            "Using cached https://download.pytorch.org/whl/numpy-2.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.9 MB)\n",
            "Installing collected packages: mpmath, triton, sympy, pillow, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, numpy, networkx, MarkupSafe, fsspec, filelock, nvidia-cusolver-cu11, nvidia-cudnn-cu11, jinja2, torch, torchvision, torchaudio\n",
            "Successfully installed MarkupSafe-2.1.5 filelock-3.19.1 fsspec-2025.9.0 jinja2-3.1.3 mpmath-1.3.0 networkx-3.5 numpy-2.3.3 nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-9.1.0.70 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.21.5 nvidia-nvtx-cu11-11.8.86 pillow-11.3.0 sympy-1.14.0 torch-2.7.1+cu118 torchaudio-2.7.1+cu118 torchvision-0.22.1+cu118 triton-3.3.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[?25hCollecting scenedetect\n",
            "  Downloading scenedetect-0.6.7.1-py3-none-any.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.9/130.9 kB\u001b[0m \u001b[31m574.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting opencv-python\n",
            "  Downloading opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (67.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /home/khushpatel/venv/lib/python3.11/site-packages (4.67.1)\n",
            "Requirement already satisfied: pillow in /home/khushpatel/venv/lib/python3.11/site-packages (11.3.0)\n",
            "Requirement already satisfied: filelock in /home/khushpatel/venv/lib/python3.11/site-packages (from transformers) (3.19.1)\n",
            "Collecting huggingface-hub<1.0,>=0.34.0\n",
            "  Downloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /home/khushpatel/venv/lib/python3.11/site-packages (from transformers) (2.3.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/khushpatel/venv/lib/python3.11/site-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/khushpatel/venv/lib/python3.11/site-packages (from transformers) (6.0.3)\n",
            "Collecting regex!=2019.12.17\n",
            "  Downloading regex-2025.11.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (800 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.4/800.4 kB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /home/khushpatel/venv/lib/python3.11/site-packages (from transformers) (2.32.5)\n",
            "Collecting tokenizers<=0.23.0,>=0.22.0\n",
            "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m127.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.4.3\n",
            "  Downloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.2/507.2 kB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting click<8.3.0,~=8.0\n",
            "  Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs in /home/khushpatel/venv/lib/python3.11/site-packages (from scenedetect) (4.5.0)\n",
            "Collecting numpy>=1.17\n",
            "  Downloading numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m102.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec>=2023.5.0 in /home/khushpatel/venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/khushpatel/venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Collecting hf-xet<2.0.0,>=1.1.3\n",
            "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m123.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset_normalizer<4,>=2 in /home/khushpatel/venv/lib/python3.11/site-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/khushpatel/venv/lib/python3.11/site-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/khushpatel/venv/lib/python3.11/site-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/khushpatel/venv/lib/python3.11/site-packages (from requests->transformers) (2025.11.12)\n",
            "Installing collected packages: safetensors, regex, numpy, hf-xet, click, scenedetect, opencv-python, huggingface-hub, tokenizers, transformers\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.3.3\n",
            "    Uninstalling numpy-2.3.3:\n",
            "      Successfully uninstalled numpy-2.3.3\n",
            "Successfully installed click-8.2.1 hf-xet-1.2.0 huggingface-hub-0.36.0 numpy-2.2.6 opencv-python-4.12.0.88 regex-2025.11.3 safetensors-0.7.0 scenedetect-0.6.7.1 tokenizers-0.22.1 transformers-4.57.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Optional: install runtime deps (requires internet access)\n",
        "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "%pip install transformers scenedetect opencv-python tqdm pillow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d4a94840",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import math\n",
        "import os\n",
        "import subprocess\n",
        "import uuid\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Dict, Iterable, List, Optional, Tuple\n",
        "\n",
        "import cv2\n",
        "import scenedetect as sd\n",
        "import torch\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from transformers import CLIPModel, CLIPProcessor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "6c1ba50a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset root: /home/khushpatel/shortform-video-ranker/data/1/TrainValVideo\n",
            "Index root: /home/khushpatel/shortform-video-ranker/notebook_artifacts/index\n"
          ]
        }
      ],
      "source": [
        "DATASET_ROOT = Path('data/1/TrainValVideo').resolve()\n",
        "INDEX_ROOT = Path('notebook_artifacts/index').resolve()\n",
        "TENSOR_CACHE = INDEX_ROOT / 'scene_tensors'\n",
        "INDEX_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "TENSOR_CACHE.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "VIDEO_EXTENSIONS = {'.mp4', '.mov', '.avi', '.mkv'}\n",
        "print(f'Dataset root: {DATASET_ROOT}')\n",
        "print(f'Index root: {INDEX_ROOT}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "a3fa006a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def list_video_files(root: Path, limit: Optional[int] = None) -> List[Path]:\n",
        "    files = []\n",
        "    for path in root.rglob('*'):\n",
        "        if path.suffix.lower() in VIDEO_EXTENSIONS:\n",
        "            files.append(path)\n",
        "            if limit and len(files) >= limit:\n",
        "                break\n",
        "    return sorted(files)\n",
        "\n",
        "def ffprobe_duration(path: Path) -> float:\n",
        "    try:\n",
        "        completed = subprocess.run(\n",
        "            ['ffprobe', '-v', 'error', '-show_entries', 'format=duration', '-of',\n",
        "             'default=noprint_wrappers=1:nokey=1', str(path)],\n",
        "            check=True,\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.STDOUT\n",
        "        )\n",
        "        return float(completed.stdout)\n",
        "    except Exception:\n",
        "        return 0.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "7d6ca59f",
      "metadata": {},
      "outputs": [],
      "source": [
        "class SceneFeatureExtractor:\n",
        "    def __init__(self, samples_per_scene: int = 3, threshold: float = 27.0, tensor_cache: Path = TENSOR_CACHE):\n",
        "        self.samples_per_scene = samples_per_scene\n",
        "        self.threshold = threshold\n",
        "        self.tensor_cache = Path(tensor_cache)\n",
        "        self.tensor_cache.mkdir(parents=True, exist_ok=True)\n",
        "        self.clip_processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')\n",
        "    def _collect_scenes(self, video_path: Path) -> List[Tuple[sd.FrameTimecode, sd.FrameTimecode]]:\n",
        "        video = sd.open_video(str(video_path))\n",
        "        manager = sd.SceneManager()\n",
        "        manager.add_detector(sd.ContentDetector(threshold=self.threshold))\n",
        "        manager.detect_scenes(video)\n",
        "        return manager.get_scene_list()\n",
        "    def _sample_frames(self, frame_start: int, frame_end: int) -> List[int]:\n",
        "        length = max(frame_end - frame_start, 1)\n",
        "        every_n = max(round(length / self.samples_per_scene), 1)\n",
        "        return [frame_start + min(idx * every_n, length - 1) for idx in range(self.samples_per_scene)]\n",
        "    def _save_tensor(self, tensor: torch.Tensor) -> str:\n",
        "        target = self.tensor_cache / f'{uuid.uuid4().hex}.pt'\n",
        "        torch.save(tensor.cpu(), target)\n",
        "        return str(target)\n",
        "    def extract(self, video_path: Path) -> Dict:\n",
        "        scenes = self._collect_scenes(video_path)\n",
        "        cap = cv2.VideoCapture(str(video_path))\n",
        "        if not scenes:\n",
        "            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "            class _FrameStub:\n",
        "                def __init__(self, frame_num: int):\n",
        "                    self.frame_num = int(frame_num)\n",
        "            scenes = [(_FrameStub(0), _FrameStub(max(frame_count - 1, 0)))]\n",
        "        clip_pixel_scenes = []\n",
        "        for scene_no, (start_tc, end_tc) in enumerate(scenes):\n",
        "            samples = self._sample_frames(start_tc.frame_num, end_tc.frame_num)\n",
        "            tensors = []\n",
        "            for sample in samples:\n",
        "                cap.set(cv2.CAP_PROP_POS_FRAMES, sample)\n",
        "                ok, frame = cap.read()\n",
        "                if not ok:\n",
        "                    continue\n",
        "                pil_image = Image.fromarray(frame)\n",
        "                inputs = self.clip_processor(images=pil_image, return_tensors='pt', padding=True)\n",
        "                tensors.append(inputs['pixel_values'].squeeze(0))\n",
        "            if not tensors:\n",
        "                continue\n",
        "            stacked = torch.stack(tensors)\n",
        "            tensor_path = self._save_tensor(torch.mean(stacked, dim=0))\n",
        "            clip_pixel_scenes.append({\n",
        "                'local_path': tensor_path,\n",
        "                'scene_no': scene_no,\n",
        "                'scene': {\n",
        "                    'start_frame_num': start_tc.frame_num,\n",
        "                    'end_frame_num': end_tc.frame_num,\n",
        "                }\n",
        "            })\n",
        "        cap.release()\n",
        "        return {\n",
        "            'num_of_scenes': len(clip_pixel_scenes),\n",
        "            'clip_pixel_scenes': clip_pixel_scenes,\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "eee56856",
      "metadata": {},
      "outputs": [],
      "source": [
        "class FrameTextScorer:\n",
        "    def __init__(self, device: Optional[str] = None):\n",
        "        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')\n",
        "        self.model = CLIPModel.from_pretrained('openai/clip-vit-base-patch32').to(self.device)\n",
        "\n",
        "    def score(self, tensor_paths: Iterable[str], text: str) -> float:\n",
        "        tensor_paths = list(tensor_paths)\n",
        "        if not tensor_paths:\n",
        "            return 0.0\n",
        "        total = 0.0\n",
        "        for tensor_path in tensor_paths:\n",
        "            image_tensor = torch.load(tensor_path, map_location=self.device)\n",
        "            if image_tensor.ndim == 3:\n",
        "                image_tensor = image_tensor.unsqueeze(0)\n",
        "            inputs = self.processor(text=[text], return_tensors='pt', padding=True).to(self.device)\n",
        "            inputs['pixel_values'] = image_tensor.to(self.device)\n",
        "            with torch.inference_mode():\n",
        "                outputs = self.model(**inputs)\n",
        "            probs = outputs.logits_per_image.squeeze()\n",
        "            total += probs.item()\n",
        "        return float(total / len(tensor_paths))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "db898cf4",
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class SceneRecord:\n",
        "    scene_id: str\n",
        "    tensor_path: str\n",
        "    start_frame: int\n",
        "    end_frame: int\n",
        "\n",
        "class IndexStorage:\n",
        "    def __init__(self, index_root: Path = INDEX_ROOT):\n",
        "        self.index_root = Path(index_root)\n",
        "        self.index_root.mkdir(parents=True, exist_ok=True)\n",
        "        self.metadata_path = self.index_root / 'video_metadata.json'\n",
        "        self.scene_path = self.index_root / 'video_scenes.json'\n",
        "        self.scene_records_path = self.index_root / 'scene_records.json'\n",
        "        self.uri_map_path = self.index_root / 'uri_to_id.json'\n",
        "        self._metadata = self._load(self.metadata_path)\n",
        "        self._video_scenes = self._load(self.scene_path)\n",
        "        self._scene_records = self._load(self.scene_records_path)\n",
        "        self._uri_map = self._load(self.uri_map_path)\n",
        "\n",
        "    def _load(self, path: Path) -> Dict:\n",
        "        if path.exists():\n",
        "            return json.loads(path.read_text())\n",
        "        return {}\n",
        "\n",
        "    def _save(self, data: Dict, path: Path) -> None:\n",
        "        path.write_text(json.dumps(data, indent=2))\n",
        "\n",
        "    def has_video(self, video_uri: str) -> bool:\n",
        "        return video_uri in self._uri_map\n",
        "\n",
        "    def add_video(self, video_uri: str, video_duration: float, scenes: List[SceneRecord]) -> str:\n",
        "        video_id = str(uuid.uuid4())\n",
        "        self._metadata[video_id] = {\n",
        "            'video_uri': video_uri,\n",
        "            'video_duration': video_duration,\n",
        "        }\n",
        "        self._video_scenes[video_id] = [scene.scene_id for scene in scenes]\n",
        "        for scene in scenes:\n",
        "            self._scene_records[scene.scene_id] = {\n",
        "                'tensor_path': scene.tensor_path,\n",
        "                'start_frame': scene.start_frame,\n",
        "                'end_frame': scene.end_frame,\n",
        "                'video_id': video_id,\n",
        "            }\n",
        "        self._uri_map[video_uri] = video_id\n",
        "        self._commit()\n",
        "        return video_id\n",
        "\n",
        "    def _commit(self):\n",
        "        self._save(self._metadata, self.metadata_path)\n",
        "        self._save(self._video_scenes, self.scene_path)\n",
        "        self._save(self._scene_records, self.scene_records_path)\n",
        "        self._save(self._uri_map, self.uri_map_path)\n",
        "\n",
        "    def iter_videos(self):\n",
        "        for video_id, metadata in self._metadata.items():\n",
        "            yield video_id, metadata\n",
        "\n",
        "    def scene_tensor_paths(self, video_id: str) -> List[str]:\n",
        "        scene_ids = self._video_scenes.get(video_id, [])\n",
        "        return [self._scene_records[s]['tensor_path'] for s in scene_ids if s in self._scene_records]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "3b845cc6",
      "metadata": {},
      "outputs": [],
      "source": [
        "class VideoIndexer:\n",
        "    def __init__(self, extractor: SceneFeatureExtractor, storage: IndexStorage):\n",
        "        self.extractor = extractor\n",
        "        self.storage = storage\n",
        "\n",
        "    def index_video(self, video_path: Path) -> Optional[str]:\n",
        "        video_path = Path(video_path).resolve()\n",
        "        if self.storage.has_video(str(video_path)):\n",
        "            return None\n",
        "        features = self.extractor.extract(video_path)\n",
        "        scene_records = []\n",
        "        for clip_scene in features['clip_pixel_scenes']:\n",
        "            scene_records.append(SceneRecord(\n",
        "                scene_id=str(uuid.uuid4()),\n",
        "                tensor_path=clip_scene['local_path'],\n",
        "                start_frame=clip_scene['scene']['start_frame_num'],\n",
        "                end_frame=clip_scene['scene']['end_frame_num'],\n",
        "            ))\n",
        "        duration = ffprobe_duration(video_path)\n",
        "        return self.storage.add_video(str(video_path), duration, scene_records)\n",
        "\n",
        "    def bulk_index(self, video_paths: Iterable[Path], max_items: Optional[int] = None) -> Dict[str, int]:\n",
        "        processed = 0\n",
        "        skipped = 0\n",
        "        for video_path in tqdm(video_paths, desc='Indexing videos'):\n",
        "            if max_items and processed >= max_items:\n",
        "                break\n",
        "            video_id = self.index_video(video_path)\n",
        "            if video_id:\n",
        "                processed += 1\n",
        "            else:\n",
        "                skipped += 1\n",
        "        return {'processed': processed, 'skipped': skipped}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "642c04be",
      "metadata": {},
      "outputs": [],
      "source": [
        "class VideoSearchEngine:\n",
        "    def __init__(self, storage: IndexStorage, scorer: FrameTextScorer):\n",
        "        self.storage = storage\n",
        "        self.scorer = scorer\n",
        "\n",
        "    def search(self, text: str, top_k: int = 5) -> List[Dict]:\n",
        "        results = []\n",
        "        for video_id, metadata in self.storage.iter_videos():\n",
        "            tensor_paths = self.storage.scene_tensor_paths(video_id)\n",
        "            if not tensor_paths:\n",
        "                continue\n",
        "            score = self.scorer.score(tensor_paths, text)\n",
        "            results.append({\n",
        "                'video_id': video_id,\n",
        "                'video_uri': metadata['video_uri'],\n",
        "                'video_duration': metadata['video_duration'],\n",
        "                'score': score,\n",
        "            })\n",
        "        results.sort(key=lambda item: item['score'], reverse=True)\n",
        "        return results[:top_k]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f562763b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Components instantiated.\n"
          ]
        }
      ],
      "source": [
        "scene_extractor = SceneFeatureExtractor(samples_per_scene=3)\n",
        "storage = IndexStorage(INDEX_ROOT)\n",
        "indexer = VideoIndexer(scene_extractor, storage)\n",
        "frame_scorer = FrameTextScorer()\n",
        "engine = VideoSearchEngine(storage, frame_scorer)\n",
        "print('Components instantiated.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c29890ce",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 7010 candidate video files. Showing a sample:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[PosixPath('/home/khushpatel/shortform-video-ranker/data/1/TrainValVideo/video0.mp4'),\n",
              " PosixPath('/home/khushpatel/shortform-video-ranker/data/1/TrainValVideo/video1.mp4'),\n",
              " PosixPath('/home/khushpatel/shortform-video-ranker/data/1/TrainValVideo/video10.mp4'),\n",
              " PosixPath('/home/khushpatel/shortform-video-ranker/data/1/TrainValVideo/video100.mp4'),\n",
              " PosixPath('/home/khushpatel/shortform-video-ranker/data/1/TrainValVideo/video1000.mp4')]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "video_files = list_video_files(DATASET_ROOT)\n",
        "print(f'Found {len(video_files)} candidate video files. Showing a sample:')\n",
        "video_files[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "3f264a30",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Indexing videos: 100%|██████████| 7010/7010 [1:06:26<00:00,  1.76it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'processed': 7007, 'skipped': 3}"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Index a manageable subset first to verify everything works.\n",
        "subset_to_index = video_files  # index all files\n",
        "index_summary = indexer.bulk_index(subset_to_index)\n",
        "index_summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8458f163",
      "metadata": {},
      "outputs": [],
      "source": [
        "example_query = 'a person talking to the camera'\n",
        "search_results = engine.search(example_query, top_k=10)\n",
        "search_results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39211b8c",
      "metadata": {},
      "source": [
        "## Hooking in an LLM reranker\n",
        "Once you have coarse CLIP scores, you can push the top-k clips (with metadata or transcripts) through any LLM to\n",
        "build a second-pass ranking. Provide a textual description of each candidate (scene summaries, ASR text, etc.),\n",
        "ask the model to order them by relevance, and merge the ordering back into your structured results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e5c31c8",
      "metadata": {},
      "outputs": [],
      "source": [
        "def rerank_with_llm_stub(results: List[Dict], prompt_builder):\n",
        "    # Placeholder that shows how you could integrate an LLM client.\n",
        "    # `prompt_builder` should accept the candidate list and return a prompt string.\n",
        "    # Replace the body with real LLM calls (OpenAI, Azure, local models, etc.).\n",
        "    prompt = prompt_builder(results)\n",
        "    print('LLM prompt preview:')\n",
        "    print(prompt[:500])\n",
        "    # TODO: send prompt to your model and parse its ordering.\n",
        "    # For now, just return the incoming list so you can wire this up later.\n",
        "    return results\n",
        "\n",
        "def simple_prompt_builder(results: List[Dict]) -> str:\n",
        "    lines = ['You are a helpful assistant. Rank the following video clips by relevance.']\n",
        "    for idx, item in enumerate(results, start=1):\n",
        "        lines.append(f\"Clip {idx}: URI={item['video_uri']} | Duration={item['video_duration']:.2f}s | CLIP score={item['score']:.3f}\")\n",
        "    lines.append('Respond with the clip numbers in best-to-worst order separated by commas.')\n",
        "    return '\n",
        "'.join(lines)\n",
        "\n",
        "reranked_results = rerank_with_llm_stub(search_results, simple_prompt_builder)\n",
        "reranked_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be696bdf",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f86b1168",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
