{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b55fb559",
      "metadata": {},
      "source": [
        "# Short-form Video Search Playground\n",
        "This notebook recreates the core CLIP-based video indexing/search pipeline from the `CLIP-video-search` repository,\n",
        "and adapts it so you can experiment directly on the MSRVTT clips that live under `shortform-video-ranker/data`.\n",
        "\n",
        "Use it end-to-end or cherry-pick the pieces you need for your short-form video + LLM reranking project.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3ad2817",
      "metadata": {},
      "source": [
        "## Environment setup\n",
        "Make sure the runtime has GPU access plus the following system dependencies:\n",
        "- `ffmpeg/ffprobe` (for duration metadata)\n",
        "- Python packages: `torch`, `transformers`, `opencv-python`, `scenedetect`, `tqdm`, `pillow`\n",
        "\n",
        "If anything is missing, uncomment the `%pip` cell below or install the packages however you prefer before running the rest of the notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "50776d25",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /home/khushpatel/venv/lib/python3.11/site-packages (2.7.1+cu118)\n",
            "Requirement already satisfied: torchvision in /home/khushpatel/venv/lib/python3.11/site-packages (0.22.1+cu118)\n",
            "Requirement already satisfied: torchaudio in /home/khushpatel/venv/lib/python3.11/site-packages (2.7.1+cu118)\n",
            "Requirement already satisfied: filelock in /home/khushpatel/venv/lib/python3.11/site-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /home/khushpatel/venv/lib/python3.11/site-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /home/khushpatel/venv/lib/python3.11/site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx in /home/khushpatel/venv/lib/python3.11/site-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /home/khushpatel/venv/lib/python3.11/site-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /home/khushpatel/venv/lib/python3.11/site-packages (from torch) (2025.9.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /home/khushpatel/venv/lib/python3.11/site-packages (from torch) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /home/khushpatel/venv/lib/python3.11/site-packages (from torch) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /home/khushpatel/venv/lib/python3.11/site-packages (from torch) (11.8.87)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /home/khushpatel/venv/lib/python3.11/site-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /home/khushpatel/venv/lib/python3.11/site-packages (from torch) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/khushpatel/venv/lib/python3.11/site-packages (from torch) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /home/khushpatel/venv/lib/python3.11/site-packages (from torch) (10.3.0.86)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /home/khushpatel/venv/lib/python3.11/site-packages (from torch) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /home/khushpatel/venv/lib/python3.11/site-packages (from torch) (11.7.5.86)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /home/khushpatel/venv/lib/python3.11/site-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /home/khushpatel/venv/lib/python3.11/site-packages (from torch) (11.8.86)\n",
            "Requirement already satisfied: triton==3.3.1 in /home/khushpatel/venv/lib/python3.11/site-packages (from torch) (3.3.1)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /home/khushpatel/venv/lib/python3.11/site-packages (from triton==3.3.1->torch) (66.1.1)\n",
            "Requirement already satisfied: numpy in /home/khushpatel/venv/lib/python3.11/site-packages (from torchvision) (2.2.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/khushpatel/venv/lib/python3.11/site-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/khushpatel/venv/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/khushpatel/venv/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /home/khushpatel/venv/lib/python3.11/site-packages (4.57.1)\n",
            "Requirement already satisfied: scenedetect in /home/khushpatel/venv/lib/python3.11/site-packages (0.6.7.1)\n",
            "Requirement already satisfied: opencv-python in /home/khushpatel/venv/lib/python3.11/site-packages (4.12.0.88)\n",
            "Requirement already satisfied: tqdm in /home/khushpatel/venv/lib/python3.11/site-packages (4.67.1)\n",
            "Requirement already satisfied: pillow in /home/khushpatel/venv/lib/python3.11/site-packages (11.3.0)\n",
            "Requirement already satisfied: filelock in /home/khushpatel/venv/lib/python3.11/site-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/khushpatel/venv/lib/python3.11/site-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/khushpatel/venv/lib/python3.11/site-packages (from transformers) (2.2.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/khushpatel/venv/lib/python3.11/site-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/khushpatel/venv/lib/python3.11/site-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/khushpatel/venv/lib/python3.11/site-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /home/khushpatel/venv/lib/python3.11/site-packages (from transformers) (2.32.5)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/khushpatel/venv/lib/python3.11/site-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /home/khushpatel/venv/lib/python3.11/site-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: click<8.3.0,~=8.0 in /home/khushpatel/venv/lib/python3.11/site-packages (from scenedetect) (8.2.1)\n",
            "Requirement already satisfied: platformdirs in /home/khushpatel/venv/lib/python3.11/site-packages (from scenedetect) (4.5.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /home/khushpatel/venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/khushpatel/venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/khushpatel/venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /home/khushpatel/venv/lib/python3.11/site-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/khushpatel/venv/lib/python3.11/site-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/khushpatel/venv/lib/python3.11/site-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/khushpatel/venv/lib/python3.11/site-packages (from requests->transformers) (2025.11.12)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.13.0-cp39-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.6/23.6 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<3.0,>=1.25.0 in /home/khushpatel/venv/lib/python3.11/site-packages (from faiss-cpu) (2.2.6)\n",
            "Requirement already satisfied: packaging in /home/khushpatel/venv/lib/python3.11/site-packages (from faiss-cpu) (25.0)\n",
            "Installing collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.13.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Optional: install runtime deps (requires internet access)\n",
        "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "%pip install transformers scenedetect opencv-python tqdm pillow\n",
        "%pip install faiss-cpu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "d4a94840",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import math\n",
        "import os\n",
        "import subprocess\n",
        "import uuid\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Dict, Iterable, List, Optional, Tuple\n",
        "\n",
        "import cv2\n",
        "import scenedetect as sd\n",
        "import torch\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from transformers import CLIPModel, CLIPProcessor\n",
        "import numpy as np\n",
        "import faiss\n",
        "import torch.nn.functional as F\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "6c1ba50a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset root: /home/khushpatel/shortform-video-ranker/data/1/TrainValVideo\n",
            "Index root: /home/khushpatel/shortform-video-ranker/notebook_artifacts/index\n"
          ]
        }
      ],
      "source": [
        "DATASET_ROOT = Path('data/1/TrainValVideo').resolve()\n",
        "INDEX_ROOT = Path('notebook_artifacts/index').resolve()\n",
        "TENSOR_CACHE = INDEX_ROOT / 'scene_tensors'\n",
        "FAISS_SCENE_IDS_PATH = INDEX_ROOT / 'faiss_scene_ids.json'\n",
        "FAISS_META_PATH = INDEX_ROOT / 'faiss_scene_meta.json'\n",
        "FAISS_INDEX_PATH = INDEX_ROOT / 'faiss.index'\n",
        "INDEX_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "TENSOR_CACHE.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "VIDEO_EXTENSIONS = {'.mp4', '.mov', '.avi', '.mkv'}\n",
        "print(f'Dataset root: {DATASET_ROOT}')\n",
        "print(f'Index root: {INDEX_ROOT}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "a3fa006a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def list_video_files(root: Path, limit: Optional[int] = None) -> List[Path]:\n",
        "    files = []\n",
        "    for path in root.rglob('*'):\n",
        "        if path.suffix.lower() in VIDEO_EXTENSIONS:\n",
        "            files.append(path)\n",
        "            if limit and len(files) >= limit:\n",
        "                break\n",
        "    return sorted(files)\n",
        "\n",
        "def ffprobe_duration(path: Path) -> float:\n",
        "    try:\n",
        "        completed = subprocess.run(\n",
        "            ['ffprobe', '-v', 'error', '-show_entries', 'format=duration', '-of',\n",
        "             'default=noprint_wrappers=1:nokey=1', str(path)],\n",
        "            check=True,\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.STDOUT\n",
        "        )\n",
        "        return float(completed.stdout)\n",
        "    except Exception:\n",
        "        return 0.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "7d6ca59f",
      "metadata": {},
      "outputs": [],
      "source": [
        "class SceneFeatureExtractor:\n",
        "    def __init__(self, samples_per_scene: int = 3, threshold: float = 27.0, tensor_cache: Path = TENSOR_CACHE):\n",
        "        self.samples_per_scene = samples_per_scene\n",
        "        self.threshold = threshold\n",
        "        self.tensor_cache = Path(tensor_cache)\n",
        "        self.tensor_cache.mkdir(parents=True, exist_ok=True)\n",
        "        self.clip_processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')\n",
        "    def _collect_scenes(self, video_path: Path) -> List[Tuple[sd.FrameTimecode, sd.FrameTimecode]]:\n",
        "        video = sd.open_video(str(video_path))\n",
        "        manager = sd.SceneManager()\n",
        "        manager.add_detector(sd.ContentDetector(threshold=self.threshold))\n",
        "        manager.detect_scenes(video)\n",
        "        return manager.get_scene_list()\n",
        "    def _sample_frames(self, frame_start: int, frame_end: int) -> List[int]:\n",
        "        length = max(frame_end - frame_start, 1)\n",
        "        every_n = max(round(length / self.samples_per_scene), 1)\n",
        "        return [frame_start + min(idx * every_n, length - 1) for idx in range(self.samples_per_scene)]\n",
        "    def _save_tensor(self, tensor: torch.Tensor) -> str:\n",
        "        target = self.tensor_cache / f'{uuid.uuid4().hex}.pt'\n",
        "        torch.save(tensor.cpu(), target)\n",
        "        return str(target)\n",
        "    def extract(self, video_path: Path) -> Dict:\n",
        "        scenes = self._collect_scenes(video_path)\n",
        "        cap = cv2.VideoCapture(str(video_path))\n",
        "        if not scenes:\n",
        "            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "            class _FrameStub:\n",
        "                def __init__(self, frame_num: int):\n",
        "                    self.frame_num = int(frame_num)\n",
        "            scenes = [(_FrameStub(0), _FrameStub(max(frame_count - 1, 0)))]\n",
        "        clip_pixel_scenes = []\n",
        "        for scene_no, (start_tc, end_tc) in enumerate(scenes):\n",
        "            samples = self._sample_frames(start_tc.frame_num, end_tc.frame_num)\n",
        "            tensors = []\n",
        "            for sample in samples:\n",
        "                cap.set(cv2.CAP_PROP_POS_FRAMES, sample)\n",
        "                ok, frame = cap.read()\n",
        "                if not ok:\n",
        "                    continue\n",
        "                pil_image = Image.fromarray(frame)\n",
        "                inputs = self.clip_processor(images=pil_image, return_tensors='pt', padding=True)\n",
        "                tensors.append(inputs['pixel_values'].squeeze(0))\n",
        "            if not tensors:\n",
        "                continue\n",
        "            stacked = torch.stack(tensors)\n",
        "            tensor_path = self._save_tensor(torch.mean(stacked, dim=0))\n",
        "            clip_pixel_scenes.append({\n",
        "                'local_path': tensor_path,\n",
        "                'scene_no': scene_no,\n",
        "                'scene': {\n",
        "                    'start_frame_num': start_tc.frame_num,\n",
        "                    'end_frame_num': end_tc.frame_num,\n",
        "                }\n",
        "            })\n",
        "        cap.release()\n",
        "        return {\n",
        "            'num_of_scenes': len(clip_pixel_scenes),\n",
        "            'clip_pixel_scenes': clip_pixel_scenes,\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "eee56856",
      "metadata": {},
      "outputs": [],
      "source": [
        "class FrameTextScorer:\n",
        "    def __init__(self, device: Optional[str] = None):\n",
        "        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')\n",
        "        self.model = CLIPModel.from_pretrained('openai/clip-vit-base-patch32').to(self.device)\n",
        "\n",
        "    def score(self, tensor_paths: Iterable[str], text: str) -> float:\n",
        "        tensor_paths = list(tensor_paths)\n",
        "        if not tensor_paths:\n",
        "            return 0.0\n",
        "        total = 0.0\n",
        "        for tensor_path in tensor_paths:\n",
        "            image_tensor = torch.load(tensor_path, map_location=self.device)\n",
        "            if image_tensor.ndim == 3:\n",
        "                image_tensor = image_tensor.unsqueeze(0)\n",
        "            inputs = self.processor(text=[text], return_tensors='pt', padding=True).to(self.device)\n",
        "            inputs['pixel_values'] = image_tensor.to(self.device)\n",
        "            with torch.inference_mode():\n",
        "                outputs = self.model(**inputs)\n",
        "            probs = outputs.logits_per_image.squeeze()\n",
        "            total += probs.item()\n",
        "        return float(total / len(tensor_paths))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "db898cf4",
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class SceneRecord:\n",
        "    scene_id: str\n",
        "    tensor_path: str\n",
        "    start_frame: int\n",
        "    end_frame: int\n",
        "    \n",
        "class IndexStorage:\n",
        "    def __init__(self, index_root: Path = INDEX_ROOT):\n",
        "        self.index_root = Path(index_root)\n",
        "        self.index_root.mkdir(parents=True, exist_ok=True)\n",
        "        self.metadata_path = self.index_root / 'video_metadata.json'\n",
        "        self.scene_path = self.index_root / 'video_scenes.json'\n",
        "        self.scene_records_path = self.index_root / 'scene_records.json'\n",
        "        self.uri_map_path = self.index_root / 'uri_to_id.json'\n",
        "        self._metadata = self._load(self.metadata_path)\n",
        "        self._video_scenes = self._load(self.scene_path)\n",
        "        self._scene_records = self._load(self.scene_records_path)\n",
        "        self._uri_map = self._load(self.uri_map_path)\n",
        "    def _load(self, path: Path) -> Dict:\n",
        "        if path.exists():\n",
        "            return json.loads(path.read_text())\n",
        "        return {}\n",
        "    def _save(self, data: Dict, path: Path) -> None:\n",
        "        path.write_text(json.dumps(data, indent=2))\n",
        "    def has_video(self, video_uri: str) -> bool:\n",
        "        return video_uri in self._uri_map\n",
        "    def add_video(self, video_uri: str, video_duration: float, scenes: List[SceneRecord]) -> str:\n",
        "        video_id = str(uuid.uuid4())\n",
        "        self._metadata[video_id] = {\n",
        "            'video_uri': video_uri,\n",
        "            'video_duration': video_duration,\n",
        "        }\n",
        "        self._video_scenes[video_id] = [scene.scene_id for scene in scenes]\n",
        "        for scene in scenes:\n",
        "            self._scene_records[scene.scene_id] = {\n",
        "                'tensor_path': scene.tensor_path,\n",
        "                'start_frame': scene.start_frame,\n",
        "                'end_frame': scene.end_frame,\n",
        "                'video_id': video_id,\n",
        "            }\n",
        "        self._uri_map[video_uri] = video_id\n",
        "        self._commit()\n",
        "        return video_id\n",
        "    def _commit(self):\n",
        "        self._save(self._metadata, self.metadata_path)\n",
        "        self._save(self._video_scenes, self.scene_path)\n",
        "        self._save(self._scene_records, self.scene_records_path)\n",
        "        self._save(self._uri_map, self.uri_map_path)\n",
        "    def iter_videos(self):\n",
        "        for video_id, metadata in self._metadata.items():\n",
        "            yield video_id, metadata\n",
        "    def iter_scenes(self):\n",
        "        for scene_id, record in self._scene_records.items():\n",
        "            yield scene_id, record\n",
        "    def get_video_metadata(self, video_id: str):\n",
        "        return self._metadata.get(video_id)\n",
        "    def scene_tensor_paths(self, video_id: str) -> List[str]:\n",
        "        scene_ids = self._video_scenes.get(video_id, [])\n",
        "        return [self._scene_records[s]['tensor_path'] for s in scene_ids if s in self._scene_records]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "3b845cc6",
      "metadata": {},
      "outputs": [],
      "source": [
        "class VideoIndexer:\n",
        "    def __init__(self, extractor: SceneFeatureExtractor, storage: IndexStorage):\n",
        "        self.extractor = extractor\n",
        "        self.storage = storage\n",
        "\n",
        "    def index_video(self, video_path: Path) -> Optional[str]:\n",
        "        video_path = Path(video_path).resolve()\n",
        "        if self.storage.has_video(str(video_path)):\n",
        "            return None\n",
        "        features = self.extractor.extract(video_path)\n",
        "        scene_records = []\n",
        "        for clip_scene in features['clip_pixel_scenes']:\n",
        "            scene_records.append(SceneRecord(\n",
        "                scene_id=str(uuid.uuid4()),\n",
        "                tensor_path=clip_scene['local_path'],\n",
        "                start_frame=clip_scene['scene']['start_frame_num'],\n",
        "                end_frame=clip_scene['scene']['end_frame_num'],\n",
        "            ))\n",
        "        duration = ffprobe_duration(video_path)\n",
        "        return self.storage.add_video(str(video_path), duration, scene_records)\n",
        "\n",
        "    def bulk_index(self, video_paths: Iterable[Path], max_items: Optional[int] = None) -> Dict[str, int]:\n",
        "        processed = 0\n",
        "        skipped = 0\n",
        "        for video_path in tqdm(video_paths, desc='Indexing videos'):\n",
        "            if max_items and processed >= max_items:\n",
        "                break\n",
        "            video_id = self.index_video(video_path)\n",
        "            if video_id:\n",
        "                processed += 1\n",
        "            else:\n",
        "                skipped += 1\n",
        "        return {'processed': processed, 'skipped': skipped}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "642c04be",
      "metadata": {},
      "outputs": [],
      "source": [
        "class VideoSearchEngine:\n",
        "    def __init__(self, storage: IndexStorage, scorer: FrameTextScorer):\n",
        "        self.storage = storage\n",
        "        self.scorer = scorer\n",
        "\n",
        "    def search(self, text: str, top_k: int = 5) -> List[Dict]:\n",
        "        results = []\n",
        "        for video_id, metadata in self.storage.iter_videos():\n",
        "            tensor_paths = self.storage.scene_tensor_paths(video_id)\n",
        "            if not tensor_paths:\n",
        "                continue\n",
        "            score = self.scorer.score(tensor_paths, text)\n",
        "            results.append({\n",
        "                'video_id': video_id,\n",
        "                'video_uri': metadata['video_uri'],\n",
        "                'video_duration': metadata['video_duration'],\n",
        "                'score': score,\n",
        "            })\n",
        "        results.sort(key=lambda item: item['score'], reverse=True)\n",
        "        return results[:top_k]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "f562763b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Components instantiated.\n"
          ]
        }
      ],
      "source": [
        "scene_extractor = SceneFeatureExtractor(samples_per_scene=3)\n",
        "storage = IndexStorage(INDEX_ROOT)\n",
        "indexer = VideoIndexer(scene_extractor, storage)\n",
        "frame_scorer = FrameTextScorer()\n",
        "engine = VideoSearchEngine(storage, frame_scorer)\n",
        "print('Components instantiated.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c29890ce",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 7010 candidate video files. Showing a sample:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[PosixPath('/home/khushpatel/shortform-video-ranker/data/1/TrainValVideo/video0.mp4'),\n",
              " PosixPath('/home/khushpatel/shortform-video-ranker/data/1/TrainValVideo/video1.mp4'),\n",
              " PosixPath('/home/khushpatel/shortform-video-ranker/data/1/TrainValVideo/video10.mp4'),\n",
              " PosixPath('/home/khushpatel/shortform-video-ranker/data/1/TrainValVideo/video100.mp4'),\n",
              " PosixPath('/home/khushpatel/shortform-video-ranker/data/1/TrainValVideo/video1000.mp4')]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "video_files = list_video_files(DATASET_ROOT)\n",
        "print(f'Found {len(video_files)} candidate video files. Showing a sample:')\n",
        "video_files[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "3f264a30",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Indexing videos: 100%|██████████| 7010/7010 [1:06:26<00:00,  1.76it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'processed': 7007, 'skipped': 3}"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Index a manageable subset first to verify everything works.\n",
        "subset_to_index = video_files  # index all files\n",
        "index_summary = indexer.bulk_index(subset_to_index)\n",
        "index_summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "8458f163",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'video_id': '3ad0ae0c-6ae8-47e1-9753-5ca7caa89b54',\n",
              "  'video_uri': '/home/khushpatel/shortform-video-ranker/data/1/TrainValVideo/video1758.mp4',\n",
              "  'video_duration': 0.0,\n",
              "  'score': 30.197233200073242},\n",
              " {'video_id': '2f1b9870-0c66-45be-abd7-d3e0b5204948',\n",
              "  'video_uri': '/home/khushpatel/shortform-video-ranker/data/1/TrainValVideo/video117.mp4',\n",
              "  'video_duration': 0.0,\n",
              "  'score': 29.495466232299805},\n",
              " {'video_id': 'b1057168-556b-45c0-8ca6-dfc748a266c1',\n",
              "  'video_uri': '/home/khushpatel/shortform-video-ranker/data/1/TrainValVideo/video1451.mp4',\n",
              "  'video_duration': 0.0,\n",
              "  'score': 28.711292266845703},\n",
              " {'video_id': 'd40a33dc-3813-445e-bbdb-fbe151d4193f',\n",
              "  'video_uri': '/home/khushpatel/shortform-video-ranker/data/1/TrainValVideo/video4005.mp4',\n",
              "  'video_duration': 0.0,\n",
              "  'score': 28.614532470703125},\n",
              " {'video_id': '3b77a04c-0bdf-44f3-8e1c-5ca03c59a61b',\n",
              "  'video_uri': '/home/khushpatel/shortform-video-ranker/data/1/TrainValVideo/video4701.mp4',\n",
              "  'video_duration': 0.0,\n",
              "  'score': 28.606511116027832},\n",
              " {'video_id': '50e8d137-19b5-4add-9709-94cf42056d2f',\n",
              "  'video_uri': '/home/khushpatel/shortform-video-ranker/data/1/TrainValVideo/video648.mp4',\n",
              "  'video_duration': 0.0,\n",
              "  'score': 28.524043401082356},\n",
              " {'video_id': 'a84867a7-91ce-4fd3-99d9-ae913a74293d',\n",
              "  'video_uri': '/home/khushpatel/shortform-video-ranker/data/1/TrainValVideo/video4078.mp4',\n",
              "  'video_duration': 0.0,\n",
              "  'score': 28.491350173950195},\n",
              " {'video_id': '626416e0-f8ea-4fdf-bc80-51c6cf6b0097',\n",
              "  'video_uri': '/home/khushpatel/shortform-video-ranker/data/1/TrainValVideo/video6665.mp4',\n",
              "  'video_duration': 0.0,\n",
              "  'score': 28.42875862121582},\n",
              " {'video_id': '9bb4b2ad-1461-4166-8f6e-47d7fddd4336',\n",
              "  'video_uri': '/home/khushpatel/shortform-video-ranker/data/1/TrainValVideo/video3023.mp4',\n",
              "  'video_duration': 0.0,\n",
              "  'score': 28.34073543548584},\n",
              " {'video_id': '972e1182-e5f7-4753-963e-00f21e40f5f5',\n",
              "  'video_uri': '/home/khushpatel/shortform-video-ranker/data/1/TrainValVideo/video6249.mp4',\n",
              "  'video_duration': 0.0,\n",
              "  'score': 28.21942138671875}]"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example_query = 'a person talking to the camera'\n",
        "search_results = engine.search(example_query, top_k=10)\n",
        "search_results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39211b8c",
      "metadata": {},
      "source": [
        "## Hooking in an LLM reranker\n",
        "Once you have coarse CLIP scores, you can push the top-k clips (with metadata or transcripts) through any LLM to\n",
        "build a second-pass ranking. Provide a textual description of each candidate (scene summaries, ASR text, etc.),\n",
        "ask the model to order them by relevance, and merge the ordering back into your structured results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e5c31c8",
      "metadata": {},
      "outputs": [],
      "source": [
        "def rerank_with_llm_stub(results: List[Dict], prompt_builder):\n",
        "    # Placeholder that shows how you could integrate an LLM client.\n",
        "    # `prompt_builder` should accept the candidate list and return a prompt string.\n",
        "    # Replace the body with real LLM calls (OpenAI, Azure, local models, etc.).\n",
        "    prompt = prompt_builder(results)\n",
        "    print('LLM prompt preview:')\n",
        "    print(prompt[:500])\n",
        "    # TODO: send prompt to your model and parse its ordering.\n",
        "    # For now, just return the incoming list so you can wire this up later.\n",
        "    return results\n",
        "\n",
        "def simple_prompt_builder(results: List[Dict]) -> str:\n",
        "    lines = ['You are a helpful assistant. Rank the following video clips by relevance.']\n",
        "    for idx, item in enumerate(results, start=1):\n",
        "        lines.append(f\"Clip {idx}: URI={item['video_uri']} | Duration={item['video_duration']:.2f}s | CLIP score={item['score']:.3f}\")\n",
        "    lines.append('Respond with the clip numbers in best-to-worst order separated by commas.')\n",
        "    return '\n",
        "'.join(lines)\n",
        "\n",
        "reranked_results = rerank_with_llm_stub(search_results, simple_prompt_builder)\n",
        "reranked_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be696bdf",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f86b1168",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "015ea31a",
      "metadata": {},
      "source": [
        "## Accelerating search with FAISS\n",
        "These helper cells embed each cached scene tensor once, load them into a FAISS index, and\n",
        "run top-K scene retrieval using CLIP's text encoder. The resulting scene hits are aggregated\n",
        "back to video-level recommendations before any downstream reranking.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "4730c6c6",
      "metadata": {},
      "outputs": [],
      "source": [
        "faiss_index = None\n",
        "faiss_scene_ids = []\n",
        "faiss_scene_meta = []\n",
        "faiss_scene_matrix = None\n",
        "\n",
        "\n",
        "def build_faiss_index(storage: IndexStorage, scorer: FrameTextScorer):\n",
        "    global faiss_index, faiss_scene_ids, faiss_scene_meta, faiss_scene_matrix\n",
        "    if not storage._scene_records:\n",
        "        raise ValueError('No scene records available. Index videos first.')\n",
        "    scene_ids = []\n",
        "    embeddings = []\n",
        "    metadata = []\n",
        "    for scene_id, record in storage.iter_scenes():\n",
        "        tensor_path = record['tensor_path']\n",
        "        tensor = torch.load(tensor_path, map_location=scorer.device)\n",
        "        if tensor.ndim == 3:\n",
        "            tensor = tensor.unsqueeze(0)\n",
        "        tensor = tensor.to(scorer.device)\n",
        "        with torch.inference_mode():\n",
        "            features = scorer.model.get_image_features(pixel_values=tensor)\n",
        "        features = F.normalize(features, dim=-1)\n",
        "        embeddings.append(features.squeeze(0).cpu().numpy().astype('float32'))\n",
        "        enriched = dict(record)\n",
        "        enriched['scene_id'] = scene_id\n",
        "        metadata.append(enriched)\n",
        "        scene_ids.append(scene_id)\n",
        "    matrix = np.stack(embeddings).astype('float32')\n",
        "    index = faiss.IndexFlatIP(matrix.shape[1])\n",
        "    index.add(matrix)\n",
        "    faiss_index = index\n",
        "    faiss_scene_ids = scene_ids\n",
        "    faiss_scene_meta = metadata\n",
        "    faiss_scene_matrix = matrix\n",
        "    return {'scenes_indexed': len(scene_ids), 'dimension': matrix.shape[1]}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "20f7694d",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'scenes_indexed': 23820, 'dimension': 512}"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "faiss_stats = build_faiss_index(storage, frame_scorer)\n",
        "faiss_stats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "d35e2182",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'video_id': '8f81179f-4213-49fb-a582-03c05d831cbd',\n",
              "  'video_uri': '/home/khushpatel/shortform-video-ranker/data/1/TrainValVideo/video5334.mp4',\n",
              "  'video_duration': 0.0,\n",
              "  'score': 0.30587273836135864,\n",
              "  'scene_id': 'fbe3324c-063a-4e7d-a52e-4d96f0c0cd60',\n",
              "  'scene_frames': (0, 15)},\n",
              " {'video_id': '3b671d45-9fc0-4b5e-b5ad-cc6b5e735541',\n",
              "  'video_uri': '/home/khushpatel/shortform-video-ranker/data/1/TrainValVideo/video6948.mp4',\n",
              "  'video_duration': 0.0,\n",
              "  'score': 0.30110543966293335,\n",
              "  'scene_id': '8a229896-7da4-4ef7-8f81-1a00bd409990',\n",
              "  'scene_frames': (196, 223)},\n",
              " {'video_id': 'b9ca730e-21b2-45b2-83e8-88e57da4bef2',\n",
              "  'video_uri': '/home/khushpatel/shortform-video-ranker/data/1/TrainValVideo/video3215.mp4',\n",
              "  'video_duration': 0.0,\n",
              "  'score': 0.2981030344963074,\n",
              "  'scene_id': 'e9cb4d4f-a06c-4bbe-8389-e5904ae7e2bb',\n",
              "  'scene_frames': (0, 22)},\n",
              " {'video_id': 'edd1b066-bd1e-44d8-9453-cf35b7281576',\n",
              "  'video_uri': '/home/khushpatel/shortform-video-ranker/data/1/TrainValVideo/video5736.mp4',\n",
              "  'video_duration': 0.0,\n",
              "  'score': 0.2980073094367981,\n",
              "  'scene_id': 'd2a5fb2b-a393-474f-9dd8-e367aa5ac109',\n",
              "  'scene_frames': (585, 600)},\n",
              " {'video_id': '5aa1967f-b7c7-4ee1-b9f0-d89ebda48c49',\n",
              "  'video_uri': '/home/khushpatel/shortform-video-ranker/data/1/TrainValVideo/video189.mp4',\n",
              "  'video_duration': 0.0,\n",
              "  'score': 0.2950282394886017,\n",
              "  'scene_id': '1e3a407e-7fbc-4c0e-b1ec-47dc36e73b4d',\n",
              "  'scene_frames': (128, 240)}]"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def search_videos_with_faiss(text: str, top_scene_hits: int = 20, top_videos: int = 5):\n",
        "    if faiss_index is None:\n",
        "        raise ValueError('Build the FAISS index first.')\n",
        "    text_inputs = frame_scorer.processor(text=[text], return_tensors='pt', padding=True)\n",
        "    text_inputs = {k: v.to(frame_scorer.device) for k, v in text_inputs.items()}\n",
        "    with torch.inference_mode():\n",
        "        text_features = frame_scorer.model.get_text_features(**text_inputs)\n",
        "    text_features = F.normalize(text_features, dim=-1).cpu().numpy().astype('float32')\n",
        "    k = min(top_scene_hits, faiss_index.ntotal)\n",
        "    scores, indices = faiss_index.search(text_features, k)\n",
        "    scene_hits = []\n",
        "    for score, idx in zip(scores[0], indices[0]):\n",
        "        if idx == -1:\n",
        "            continue\n",
        "        meta = faiss_scene_meta[idx]\n",
        "        scene_hits.append({\n",
        "            'scene_id': meta['scene_id'],\n",
        "            'video_id': meta['video_id'],\n",
        "            'score': float(score),\n",
        "            'start_frame': meta['start_frame'],\n",
        "            'end_frame': meta['end_frame'],\n",
        "        })\n",
        "    video_scores = {}\n",
        "    for hit in scene_hits:\n",
        "        vid = hit['video_id']\n",
        "        current = video_scores.get(vid)\n",
        "        if not current or hit['score'] > current['score']:\n",
        "            video_scores[vid] = hit\n",
        "    ranked = sorted(video_scores.values(), key=lambda item: item['score'], reverse=True)[:top_videos]\n",
        "    results = []\n",
        "    for hit in ranked:\n",
        "        metadata = storage.get_video_metadata(hit['video_id']) or {}\n",
        "        results.append({\n",
        "            'video_id': hit['video_id'],\n",
        "            'video_uri': metadata.get('video_uri'),\n",
        "            'video_duration': metadata.get('video_duration'),\n",
        "            'score': hit['score'],\n",
        "            'scene_id': hit['scene_id'],\n",
        "            'scene_frames': (hit['start_frame'], hit['end_frame']),\n",
        "        })\n",
        "    return {'scene_hits': scene_hits, 'video_results': results}\n",
        "\n",
        "faiss_search_results = search_videos_with_faiss('car', top_scene_hits=30, top_videos=5)\n",
        "faiss_search_results['video_results']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4ee56a1",
      "metadata": {},
      "source": [
        "### Persisting the FAISS index\n",
        "Save the trained FAISS structure plus scene metadata to disk so you can reload them\n",
        "without recomputing embeddings after a kernel restart.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "013a259f",
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_faiss_artifacts(index_path=FAISS_INDEX_PATH, meta_path=FAISS_META_PATH, ids_path=FAISS_SCENE_IDS_PATH):\n",
        "    if faiss_index is None:\n",
        "        raise ValueError('Build the FAISS index before saving.')\n",
        "    faiss.write_index(faiss_index, str(index_path))\n",
        "    Path(meta_path).write_text(json.dumps(faiss_scene_meta))\n",
        "    Path(ids_path).write_text(json.dumps(faiss_scene_ids))\n",
        "    return {\n",
        "        'index_path': str(index_path),\n",
        "        'meta_path': str(meta_path),\n",
        "        'ids_path': str(ids_path),\n",
        "        'entries': len(faiss_scene_ids),\n",
        "    }\n",
        "\n",
        "\n",
        "def load_faiss_artifacts(index_path=FAISS_INDEX_PATH, meta_path=FAISS_META_PATH, ids_path=FAISS_SCENE_IDS_PATH):\n",
        "    global faiss_index, faiss_scene_meta, faiss_scene_ids\n",
        "    if not Path(index_path).exists():\n",
        "        raise FileNotFoundError(index_path)\n",
        "    faiss_index = faiss.read_index(str(index_path))\n",
        "    faiss_scene_meta = json.loads(Path(meta_path).read_text())\n",
        "    faiss_scene_ids = json.loads(Path(ids_path).read_text())\n",
        "    return {\n",
        "        'index_path': str(index_path),\n",
        "        'entries': len(faiss_scene_ids),\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf342786",
      "metadata": {},
      "outputs": [],
      "source": [
        "saved_faiss = save_faiss_artifacts()\n",
        "saved_faiss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5e986db",
      "metadata": {},
      "outputs": [],
      "source": [
        "loaded_faiss = load_faiss_artifacts()\n",
        "loaded_faiss\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
